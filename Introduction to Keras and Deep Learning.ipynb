{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras and Deep Learning\n",
    "\n",
    "This notebook is an introduction to the Keras framework, using tensorflow as the backend engine, and an introduction to Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup\n",
    "\n",
    "If you want to following along with the code examples you will need to setup a Python 3.6.x environment. \n",
    "\n",
    "**NOTE** Python 3.7.x is not yet supported by tensorflow and some of the other libraries.\n",
    "\n",
    "### pip install\n",
    "\n",
    "```python\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "pip install tensorflow\n",
    "pip install keras\n",
    "pip install matplotlib\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "on macos:  source venv/bin/activate to activate the python virtual environment\n",
    "\n",
    "### Jupyter Notebook Extensions\n",
    "If you would like to include jupyter notebook extensions you can execute the following:\n",
    "```python\n",
    "pip install jupyter_nbextensions_configurator jupyter_contrib_nbextensions\n",
    "jupyter contrib nbextension install --user\n",
    "jupyter nbextensions_configurator enable --user\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLearning vs ShallowLearning\n",
    "\n",
    "The most significant difference, in this authors humble opinion, between the two styles of machine learning is that in deep learning accounts for the **interaction** between the features from the data, the algorithm figures out the important features by adjusting weights, and uses neural networks to accomplish this.  \n",
    "\n",
    "In shallow learning, the machine learning engineer has to go through a step, sometimes a lengthy step, to do the feature engineering from the data.  The shallow learning model is then parameterized and tested by the machine learning engineer.  In deep learning, the algorithm typically determines the features of interest, and the interactions between the features by a series of forward propagation of data, loss function error measurement, backpropagation of the error, adjust the model weights, rinse, repeat.  \n",
    "\n",
    "This is not to say that with deep learning there is no feature engineering, or no model parameterization - but in deep learning the 'neural network' figures this out for the most part.\n",
    "\n",
    "### Feature interaction between Shallow an Deep Learning\n",
    "In shallow learning, like linear regression, each feature is considered individually, with a weight also known as coefficient to create a equation like:\n",
    "\n",
    "The general form of a linear equation used for predictions with multiple features is:\n",
    "\n",
    "**<font size=\"3\">General form of shallow learning linear regression</font>**\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
    "\n",
    "- $y$ is the response\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature)\n",
    "\n",
    "In this case:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times TV Sales + \\beta_2 \\times Radio Sales + \\beta_3 \\times Newspaper Sales$\n",
    "\n",
    "The $\\beta$ values are called the **model coefficients**. These values are \"learned\" during the model fitting step using the \"least squares\" criterion. Then, the fitted model can be used to make predictions!\n",
    "\n",
    "Shallow learning is implemented in machine learning frameworks such as [Scikit-learn](https://scikit-learn.org/stable/).\n",
    "\n",
    "\n",
    "In **DeepLearning**, all of the features are combined in a series of *hidden* nodes with weights, similar to shallow learning, however for each node the weights are adjusted based on the error to determine an optimal combination of features.\n",
    "\n",
    "Overly simplistically it might look like:\n",
    "\n",
    "$y = (\\beta_4x_1+\\beta_5x_2+\\beta_6x_3) + (\\beta_7x_1+\\beta_8x_2+\\beta_9x_3) + ... +$\n",
    "\n",
    "Deep Learning is implemented in machine learning frameworks such as [Keras](https://keras.io)\n",
    "\n",
    "Both types of Machine Learning have their place.  One is not necessarily superiour to the other.  They are both valid tools to solve a problem, and when applied correctly can achieve very good results.\n",
    "\n",
    "It seems that in 'the wild' DeepLearning is considered a better choice, because anything you can solve with ShallowLearning you can do with DeepLearning.  \n",
    "\n",
    "I have read that always applying DeepLearning to a problem is like using a sledgehammer as your only hammer.  Need to tack down upholstery - sledgehammer.  Need to nail a finishing nail into trim - sledgehammer.  Need to hang a picture on a wall - sledgehammer.  Need to break up concrete, or a drive a concrete nail into a wall - NOW... sledgehammer.  While you can use a sledgehammer for all of these problems - it might not always be the best tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Learning - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Bank Transactions = \\beta_0 + \\beta_1 \\times Age + \\beta_2 \\times Bank Balance + \\beta_3 \\times 401k Balance + \\beta_4 \\times Retire Status$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slide2](./docs/slide_images/slide_images.001.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slide3](./docs/slide_images/slide_images.002.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "\n",
    "What is:\n",
    "\n",
    "$(\\beta_4x_1+\\beta_5x_2+\\beta_6x_3)$\n",
    "\n",
    "That is the dot product of two matrices:\n",
    "\n",
    "$$\\begin{bmatrix} \\beta_4 & \\beta_5 & \\beta_6 \\end{bmatrix} dot \\begin{bmatrix} x_1 \\\\ x_2\\\\ x_3 \\end{bmatrix}$$\n",
    "\n",
    "The value of each hidden node is the dot product of the node values going into the hidden node and the weights of all edges connecting them.  This dot product, along with the *activation function* make up the *forward propagation* algorithm to determine the value of each node.\n",
    "\n",
    "Deep Learning neural networks and linear regression problems rely on linear algebra and heavily on matrix mathematics. Fortunately - the machine learning libraries handle all of this math but it is still very helpful to understand that is what is happening.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.595182Z",
     "start_time": "2019-02-02T01:17:46.590495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56 56\n"
     ]
    }
   ],
   "source": [
    "# Using Python to calculate dot products\n",
    "import numpy as np\n",
    "\n",
    "m1 = np.array([2,3,4])\n",
    "m2 = np.array([5,6,7])\n",
    "\n",
    "# m1 dot m2 = 2*5 + 3*6 + 4*7 = 56\n",
    "\n",
    "v1 = m1@m2\n",
    "v2 = np.dot(m1,m2)\n",
    "v3 = (m1*m2).sum()\n",
    "print(v1,v2,v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks use a forward propagation algorithm to make initial predictions based on the data.\n",
    "\n",
    "Lets look at an example where we want to predict the number of transations a person will make at a bank given just the number of children and the number of accounts.\n",
    "\n",
    "Forward propagation starts at the input layer, moves through the hidden layers, and then finally to the output layer.\n",
    "\n",
    "Forward propagation procedes for each data point.\n",
    "\n",
    "**NOTE** the diagram below is focused on the node value calculations *without* an activation function.  We will see how activation functions are used in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slide3](./docs/slide_images/slide_images.003.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer feature vector can be represented by:\n",
    "\n",
    "$$\\begin{bmatrix} 2 & 3 \\end{bmatrix}$$\n",
    "\n",
    "The weights vector for Node 0:\n",
    "\n",
    "$$\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "The weights vector for Node 1:\n",
    "\n",
    "$$\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$$\n",
    "\n",
    "The weights vector for the Output layer:\n",
    "\n",
    "$$\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.601363Z",
     "start_time": "2019-02-02T01:17:46.597917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# input_data vector: [2,3]\n",
    "# weight vector: [2,1]\n",
    "node_0 = np.array([2,3])@np.array([2,1])\n",
    "print(node_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.607346Z",
     "start_time": "2019-02-02T01:17:46.603946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "node_1 = np.array([2,3])@np.array([1,-1])\n",
    "print(node_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.612659Z",
     "start_time": "2019-02-02T01:17:46.609615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "target = np.array([node_0,node_1])@np.array([1,-1])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product calculation between the inputs and the weights is only part of what happens at each hidden node.\n",
    "\n",
    "Activation functions are run in the hiddlen layers and capture non-linearities in the input features. Activation functions are applied to values coming into a node.\n",
    "\n",
    "Between the combining of all of the features with various weights so that the algorithm can account for feature interactions, and the ability to capture non-linearities with activation functions is what gives deep learning its power.\n",
    "\n",
    "The value of the node can be thought of as:\n",
    "\n",
    "<font size=\"4\">node_value = activation_function(input_values dot weights )</font>\n",
    "\n",
    "A good overview of Activations function can be found in this [Toward Data Science](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many activation functions, such as *tanh*, but the ReLU function seems to be the popular one with neural networks.\n",
    "\n",
    "![ReLU](./docs/relu.png)\n",
    "\n",
    "If the value into the ReLU activation function is negative, then the output is set to zero.  Otherwise the value is returned.  \n",
    "\n",
    "It is a very simple function, which is also very effective and powerful.\n",
    "\n",
    "```python\n",
    "def relu(input):\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "\n",
    "    # Return the value just calculated\n",
    "    return (output)\n",
    "```\n",
    "\n",
    "Lets apply the ReLU activation to the example above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU Activation Example](./docs/slide_images/slide_images.004.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.617271Z",
     "start_time": "2019-02-02T01:17:46.614375Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "\n",
    "    # Return the value just calculated\n",
    "    return (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.622034Z",
     "start_time": "2019-02-02T01:17:46.618632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "node_0 = relu(np.array([2,3])@np.array([2,1]))\n",
    "print(node_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.627452Z",
     "start_time": "2019-02-02T01:17:46.624433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "node_1 = relu(np.array([2,3])@np.array([1,-1]))\n",
    "print(node_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice now, for Node 1 - the value was previously -1, however as a result of the ReLU activation function the value of Node 1 is now 0 (zero).\n",
    "\n",
    "**Note** The output layer does not necessarily use the same activation function as the hidden layers.  \n",
    "\n",
    "For the output layer typically for a regression problem you use a linear activation function and for classification one called, 'softmax'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.632661Z",
     "start_time": "2019-02-02T01:17:46.629577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "target = np.array([node_0,node_1])@np.array([1,-1])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Forward Propagation with the ReLU activation function, lets calculate the output layer values for different input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T13:32:52.339579Z",
     "start_time": "2019-02-02T13:32:52.334638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define predict_with_network()\n",
    "# this 'network' has one hidden layer with 2 nodes.\n",
    "def predict_with_network(input_data_row, weights):\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = input_data_row @ weights['node_0']\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = input_data_row @ weights['node_1']\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "    # Calculate model output\n",
    "    input_to_final_layer = hidden_layer_outputs @ weights['output']\n",
    "    \n",
    "    # note in this case we are using a relu because you cannot have negative transactions\n",
    "    model_output = relu(input_to_final_layer)\n",
    "\n",
    "    # Return model output\n",
    "    return (model_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.644701Z",
     "start_time": "2019-02-02T01:17:46.639895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 0, 0, 16]\n"
     ]
    }
   ],
   "source": [
    "# Setup input_data/features/input layer\n",
    "input_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n",
    "\n",
    "# Setup network weights\n",
    "weights = {'node_0': np.array([2, 1]), 'node_1': np.array([ 1, -1]), 'output': np.array([1, -1])}\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "\n",
    "# For each set of feature values, calculate a prediction.\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    model_output = predict_with_network(input_data_row, weights)\n",
    "    results.append(model_output)\n",
    "\n",
    "# Print results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real power of a neural network comes from the ability to have multiple hidden layers, where each layer can have a different number of nodes.\n",
    "\n",
    "Even though you might have multiple layers with a different number of nodes, the forward propagation algorithm still works the same way.\n",
    "\n",
    "- deep networks internally build representations of patterns in the data\n",
    "- partially replace the need for feature engineering\n",
    "- subsequent layers build increasingly sophisticated representations of the raw data because each prior layer provides more information on the feature interactions.\n",
    "\n",
    "Note again, that we do not program the increased representation of the interactions, the network determines this.  This is where something called *back propagation* comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with 2 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2 Hidden Layers ](./docs/slide_images/slide_images.005.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2 Hidden Layers Solution](./docs/slide_images/slide_images.006.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.651643Z",
     "start_time": "2019-02-02T01:17:46.646139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prediction of # transactions: 24\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3, 4])\n",
    "weights = {'node_0_0': np.array([2, 1]), 'node_0_1': np.array([ 1, 1]), 'node_1_0': np.array([1,2]), 'node_1_1': np.array([-2,-1]), 'output': np.array([1, -1])}\n",
    "\n",
    "node_0 = relu(input_data @ weights['node_0_0'])\n",
    "node_1 = relu(input_data @ weights['node_0_1'])\n",
    "\n",
    "hidden_layer_0 = np.array([node_0, node_1])\n",
    "\n",
    "node2 = relu(hidden_layer_0 @ weights['node_1_0'])\n",
    "node3 = relu(hidden_layer_0 @ weights['node_1_1'])\n",
    "\n",
    "hidden_layer_1 = np.array([node2, node3])\n",
    "\n",
    "target = hidden_layer_1 @ weights['output']\n",
    "\n",
    "print(f'Target prediction of # transactions: {target}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the simple network below:\n",
    "\n",
    "![Weights](./docs/slide_images/slide_images.007.jpeg)\n",
    "\n",
    "Lets also assume that we know for 2 Children, and 3 Accounts that the expected number of transactions is 10.\n",
    "\n",
    "What weights do we need to predict 10 so that the prediction error ( predicted value - target value ) is zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.656601Z",
     "start_time": "2019-02-02T01:17:46.653085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 14, Error: 4\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3,4])\n",
    "weights = {'node_0': np.array([1,1]), 'node_1': np.array([1,1]), 'output': np.array([1,1])}\n",
    "\n",
    "prediction = predict_with_network(input_data, weights)\n",
    "target = 10           \n",
    "print(f'Prediction: {prediction}, Error: {prediction - target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.669834Z",
     "start_time": "2019-02-02T01:17:46.658111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ SOLUTION found in 278 attempts ****************\n",
      "{'node_0': array([4, 7]), 'node_1': array([-2,  9]), 'output': array([-5,  7])}\n"
     ]
    }
   ],
   "source": [
    "# brute force?\n",
    "from random import randint\n",
    "squared_error_values = []\n",
    "for i in range(0,1000):\n",
    "    weights['node_0'] = np.array([randint(-10,10),randint(-10,10)])\n",
    "    weights['node_1'] = np.array([randint(-10,10),randint(-10,10)])\n",
    "    weights['output'] = np.array([randint(-10,10),randint(-10,10)])\n",
    "    \n",
    "    prediction = predict_with_network(input_data, weights)\n",
    "#     print(f'Prediction: {prediction}, Error: {prediction - target}')\n",
    "    error = prediction - target\n",
    "    if error == 0:\n",
    "        print(f\"************ SOLUTION found in {i} attempts ****************\")\n",
    "        print(weights)\n",
    "        break\n",
    "    squared_error_values.append(np.square(error))\n",
    "else:\n",
    "    print(\"Could not find solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making accurate predictions gets much more difficult as the number of hidden layers, and nodes increases.\n",
    "\n",
    "Every set of weights can have a different error value, and many values of the weights can also solve the problem.\n",
    "\n",
    "Solution:  Use a **loss function** to aggregate all of the errors into a single score.\n",
    "\n",
    "The loss function represents a measure (or score) the of models predictive performance.\n",
    "\n",
    "**For regression, mean squared error is a typical loss function.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T17:14:22.484107Z",
     "start_time": "2019-02-01T17:14:22.479560Z"
    }
   },
   "source": [
    "The lower the loss function value, the better the model.  Therefore, the lower the *mean squared error* the better the prediction.\n",
    "\n",
    "The goal is to find a set of weights that gives the lowest value for the loss function.\n",
    "\n",
    "This is done using **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is the process of finding the lowest value of a curve.  \n",
    "\n",
    "A very good article on gradient descent can be found on [Towards Data Science](https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3)\n",
    "\n",
    "![GD](./docs/gradient_descent.png)\n",
    "\n",
    "Calculate the slope of the tangent line of the loss_function for the given data points.  If that slope is positive, you decrease the value of 'w'.  If the slope is negative, you increase the value of 'w'.  \n",
    "\n",
    "Recall that the slope is the 1st derivative of the loss function.\n",
    "\n",
    "The amount you increase or decrease is governed by the **learning rate**.  \n",
    "\n",
    "The learning rate is a multiplier of the magnitude of the slope and that determines the step size:\n",
    "\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "$step\\ size = w - (learning\\ rate * slope)$\n",
    "</center>\n",
    "\n",
    "\n",
    "Where learning rate values are in the range of .1 to .001. \n",
    "\n",
    "The smaller the value of the learning rate the longer a model will take to train.  Too large a value for learning rate may cause the training to never converge because you will continually step over the minimum value.\n",
    "\n",
    "#### Slope of / Derivative of Mean Squared Error Loss Function\n",
    "\n",
    "Slope of the mean-squared error loss function with respect to a particular prediction or node value is:\n",
    "\n",
    "$slope = 2 * ( predicted\\ value - actual\\ value)$\n",
    "\n",
    "or\n",
    "\n",
    "$slope = 2 * error\\ value$\n",
    "\n",
    "<font size=\"1\">Recall that the derivative of X^2 is 2X</font>\n",
    "    \n",
    "\n",
    "#### Slope of / Derivative of ReLU Activation Function\n",
    "See [James McCaffrey Blog](https://jamesmccaffrey.wordpress.com/2017/06/23/two-ways-to-deal-with-the-derivative-of-the-relu-function/) for details.\n",
    "\n",
    "\"If x is greater than 0 the derivative is 1 and if x is less than zero the derivative is 0. But when x = 0, the derivative does not exist.\n",
    "\n",
    "There are two ways to deal with this. First, you can just arbitrarily assign a value for the derivative of y = ReLU(x) when x = 0. Common arbitrary values are 0, 0.5, and 1. Easy!\"\n",
    "\n",
    "For the not so easy - see reference above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated weights calculation Example between 2 nodes\n",
    "\n",
    "Keep in mind that when we reference the value of the 'slope of the loss function' at a particular node, that is the same as saying the value of the 'derivative of the loss function' at a particular node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SlopeCalc](./docs/slide_images/slide_images.008.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Slope of loss function with respect to value at target node\n",
    "\n",
    "recall the derivative of the mean squared loss function is '2 * error'\n",
    "\n",
    "2 * (predicted value-target value) = 2 * (target node value-target value ) = 2 * (6-10) = -8\n",
    "\n",
    "##### Value of Source Node\n",
    "\n",
    "3\n",
    "\n",
    "##### Slope of activation function with respect to value at source node\n",
    "\n",
    "1 (Since the value is positive)\n",
    "\n",
    "##### Assume a learning rate of 0.01\n",
    "\n",
    "The calculation to determine the new weight is:\n",
    "\n",
    "derivative_of_relu(source node) * source node * derivative_of_mse(error) * learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.674388Z",
     "start_time": "2019-02-02T01:17:46.671312Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24\n"
     ]
    }
   ],
   "source": [
    "#new_weight = w - (1 * source_node_value * (2 * (predicted-target) * LR)  )\n",
    "new_weight = 2 - 1 * 3 * -8 * 0.01\n",
    "print(new_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Repeat this for all of the nodes to update the weights and then perform forward propagation and measure error, perform back-propagation, rinse and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Node Example\n",
    "![2SlopeCalc](./docs/slide_images/slide_images.009.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T13:49:34.043498Z",
     "start_time": "2019-02-02T13:49:33.915541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 11\n",
      "Error: 1\n",
      "Gradient: [6 8]\n",
      "Updated Weights: [0.94 1.92]\n",
      "Updated Prediction: 10.5\n",
      "Updated Error: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "source_data = np.array([3,4])\n",
    "\n",
    "weights = np.array([1,2])\n",
    "\n",
    "target = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "preds = source_data @ weights\n",
    "\n",
    "print(f\"Prediction: {preds}\")\n",
    "\n",
    "error = preds - target\n",
    "print(f'Error: {error}')\n",
    "\n",
    "# gradient is the matrix form of the 3 steps above\n",
    "gradient = 2 * error * source_data\n",
    "print(f'Gradient: {gradient}')\n",
    "\n",
    "# new weights\n",
    "weights_updated = weights - (learning_rate * gradient)\n",
    "print(f'Updated Weights: {weights_updated}')\n",
    "\n",
    "pred_updated = source_data @ weights_updated\n",
    "print(f'Updated Prediction: {pred_updated}')\n",
    "\n",
    "error_updated = pred_updated - target\n",
    "print(f'Updated Error: {error_updated}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-Propagation\n",
    "\n",
    "Allows gradient descent to update all weights in a neural network, by calculating the graidents for all weights.\n",
    "\n",
    "Relies on chain rule from calculus but this notebook will focus on understanding the general process.\n",
    "\n",
    "A good article on Back-Propagation: [Towards Data Science](https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)\n",
    "It is actually a bit more complex than this notebook will get into.\n",
    "\n",
    "A model always performs forward propagation to get a prediction, and calculate the error.  Then the model will perform back-propagation to calculate new weights as we dicussed previously.\n",
    "\n",
    "Back-propagation goes back one *layer* at a time.  For each layer, we calculate the gradient as we did above.\n",
    "\n",
    "Slope of node values are the sum of the slopes for all weights that come out of them.\n",
    "\n",
    "* Start with random values for weights\n",
    "\n",
    "* Use forward propagation to make a prediction and calculate the error\n",
    "\n",
    "* Use backward propagation to calculate the slope of the loss function with respect to each weight\n",
    "\n",
    "* Multiple that slope by the learning rate and subtract the value from the current weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BackProp](./docs/slide_images/slide_images.010.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:46.689408Z",
     "start_time": "2019-02-02T01:17:46.685849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.82\n"
     ]
    }
   ],
   "source": [
    "#new_weight = w - (1 * source_node_value * (2 * (predicted-target) * LR)  )\n",
    "d_relu = 1\n",
    "slope_error = 2*(7-4)\n",
    "LR = 0.01\n",
    "node_value = 3\n",
    "existing_weight = 2\n",
    "new_weight = existing_weight - d_relu * node_value * slope_error * LR\n",
    "print(new_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BackProp](./docs/slide_images/slide_images.011.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BackProp](./docs/slide_images/slide_images.012.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out all of details on Keras from the [website](http://keras.io)\n",
    "\n",
    "The steps\n",
    "\n",
    "- Specify the model architecture\n",
    "\n",
    "This step will create the model and define the layers, from input layer, to any number of hidden layers, to an output layer. \n",
    "\n",
    "This architecture is:\n",
    "- single input layer\n",
    "\n",
    "- 2 hidden layers.  One with 50 nodes, the other with 32 nodes\n",
    "\n",
    "- single output layer with a single node\n",
    "\n",
    "```python\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "# notice n_cols.  This is the number of features or columns in a dataframe.\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "# single node for the predictions of the model.\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "- compile\n",
    "\n",
    "Sets up the model to get ready for the fit method.\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "Keras has many different optimizers, which essentially handle the gradient descent and back-propagation.  The 'adam' optimizer dynamically adjusts the learning rate. \n",
    "\n",
    "For regression, the loss function is generally *mean_squared_error*.  For classifications problems we would use, *categorical_crossentropy*\n",
    "\n",
    "- fit\n",
    "\n",
    "Apply backpropagation and gradient descent with data to update the weigts.\n",
    "\n",
    "The fit method takes an epoch parameter which tells Keras how many times to train the model on the data set.\n",
    "\n",
    "```python\n",
    "model.fit(X,y, epochs=10)\n",
    "```\n",
    "\n",
    "\n",
    "- predict\n",
    "\n",
    "This method will make predictions given the features and the target values.\n",
    "\n",
    "```python\n",
    "y_pred = model.predict(pred_data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because training can take a long time, once a model is trained you will want to save the model and reload it later to make predictions.  The models in this notebook are small - but even with small datasets this training can take some time.\n",
    "\n",
    "#### Save\n",
    "```python\n",
    "# Save model for later\n",
    "model.save('keras_titanic_model.h5')\n",
    "```\n",
    "\n",
    "#### Load\n",
    "```python\n",
    "model = load_model('keras_titanic_model.h5')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hourly_wage dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:47.745659Z",
     "start_time": "2019-02-02T01:17:46.691324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:17:47.769644Z",
     "start_time": "2019-02-02T01:17:47.747183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.75</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.35</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.50</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.50</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.25</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           8.75      0             12               9   27       0     0   \n",
       "1          11.35      1             12              17   35       0     1   \n",
       "2          11.50      1             12              19   37       0     0   \n",
       "3           6.50      0              8              27   41       0     1   \n",
       "4           6.25      1              9              30   45       0     0   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              0             0  \n",
       "1      0              0             0  \n",
       "2      0              1             0  \n",
       "3      1              0             0  \n",
       "4      1              0             0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/hourly_wages.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** we will see how to handle the categorical columns (union, construction, manufacturing)\n",
    "soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:18:24.641074Z",
     "start_time": "2019-02-02T01:18:24.182120Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
      "0           8.75      0             12               9   27       0     0   \n",
      "1          11.35      1             12              17   35       0     1   \n",
      "2          11.50      1             12              19   37       0     0   \n",
      "3           6.50      0              8              27   41       0     1   \n",
      "4           6.25      1              9              30   45       0     0   \n",
      "\n",
      "   south  manufacturing  construction  \n",
      "0      0              0             0  \n",
      "1      0              0             0  \n",
      "2      0              1             0  \n",
      "3      1              0             0  \n",
      "4      1              0             0  \n",
      "(525, 10)\n",
      "Actual vs Predictions:\n",
      "(5.1, 4.514681)\n",
      "(4.95, 5.8179517)\n",
      "(6.67, 6.6517305)\n",
      "(4.0, 7.4844313)\n",
      "(7.5, 9.474591)\n",
      "(13.07, 10.2209425)\n",
      "(4.45, 7.370039)\n",
      "(19.47, 8.56487)\n",
      "(13.28, 12.070466)\n",
      "RMSE: 4.13877512626986\n"
     ]
    }
   ],
   "source": [
    "KERAS_WAGE_MODEL_H_ = 'keras_wage_model_1.h5'\n",
    "\n",
    "df = pd.read_csv('./data/hourly_wages.csv')\n",
    "df_test = pd.read_csv('./data/hourly_wages_test.csv')\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "X = df.drop(columns=['wage_per_hour'])\n",
    "y = df['wage_per_hour']\n",
    "\n",
    "X_test = df_test.drop(columns=['wage_per_hour'])\n",
    "y_test = df_test['wage_per_hour']\n",
    "\n",
    "# number of features or number of inputs to the keras model\n",
    "n_cols = X.shape[1]\n",
    "\n",
    "if os.path.exists(KERAS_WAGE_MODEL_H_):\n",
    "    model = load_model(KERAS_WAGE_MODEL_H_)\n",
    "else:\n",
    "    # Set up the model: model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the first layer\n",
    "    model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "    # Add the second layer\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # scaling data before fitting can ease optimization\n",
    "\n",
    "    # fit will perform the backpropagation and gradient descent\n",
    "    # fit(X_train, y_train)\n",
    "    model.fit(X, y, epochs=100)\n",
    "\n",
    "    model.save(KERAS_WAGE_MODEL_H_)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(f'Actual vs Predictions:')\n",
    "print(*list(zip(y_test, predictions.flatten())), sep=\"\\n\")\n",
    "error = predictions.flatten() - y_test\n",
    "print(f'RMSE: {np.sqrt(np.mean(np.square(error)))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification we need to change a couple of things.\n",
    "\n",
    "- Set loss='categorical_crossentropy'\n",
    "Similar to regression loss, the lower the number the better.\n",
    "\n",
    "- Add metrics=['accuracy'] to compile options\n",
    "This will provide familiar accuracy scores when fitting.\n",
    "\n",
    "- Output layer has a separate node for each possible outcome\n",
    "\n",
    "- Add activation='softmax' for the Output layer\n",
    "Ensures the output sums to one so it can be interpreted as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncode Target values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to have single column as the target or outcome.  For example in the *titanic* data set, the *survived* target column has a 1 for survived and a 0 for not.  For Keras, we would want to one hot encode these values such that there is a Survived_Yes and a Survived_No column with a 1 in the appropriate column.  \n",
    "\n",
    "Now our single column target is a multi-column.\n",
    "\n",
    "In the output layer, we would create a Dense object and specify the number of possible outputs, and in this example that would be 2.\n",
    "\n",
    "Keras provides a function to perform the one-hot encoding called, *to_category(y)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:32:43.012756Z",
     "start_time": "2019-02-02T01:32:43.003930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0       0\n",
       "1       1\n",
       "2       2\n",
       "3       2\n",
       "4       1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'target': [0,1,2,2,1,0,0,0,0,1,2,1,2,2,2,1,1,1,0,0]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T01:33:58.357267Z",
     "start_time": "2019-02-02T01:33:58.345133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2\n",
       "0   1.0  0.0  0.0\n",
       "1   0.0  1.0  0.0\n",
       "2   0.0  0.0  1.0\n",
       "3   0.0  0.0  1.0\n",
       "4   0.0  1.0  0.0\n",
       "5   1.0  0.0  0.0\n",
       "6   1.0  0.0  0.0\n",
       "7   1.0  0.0  0.0\n",
       "8   1.0  0.0  0.0\n",
       "9   0.0  1.0  0.0\n",
       "10  0.0  0.0  1.0\n",
       "11  0.0  1.0  0.0\n",
       "12  0.0  0.0  1.0\n",
       "13  0.0  0.0  1.0\n",
       "14  0.0  0.0  1.0\n",
       "15  0.0  1.0  0.0\n",
       "16  0.0  1.0  0.0\n",
       "17  0.0  1.0  0.0\n",
       "18  1.0  0.0  0.0\n",
       "19  1.0  0.0  0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_cat = to_categorical(df)\n",
    "df_cat = pd.DataFrame(y_cat)\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras on the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the local titanic dataset, which has been cleaned up, use a Keras Deep Learning model to predict who will survive.\n",
    "\n",
    "The structure of this model will be a single hidden layer with 100 nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T02:22:46.581439Z",
     "start_time": "2019-02-02T02:22:46.562446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass   age  sibsp  parch     fare  male  age_was_missing  \\\n",
      "0         0       3  22.0      1      0   7.2500     1            False   \n",
      "1         1       1  38.0      1      0  71.2833     0            False   \n",
      "2         1       3  26.0      0      0   7.9250     0            False   \n",
      "3         1       1  35.0      1      0  53.1000     0            False   \n",
      "4         0       3  35.0      0      0   8.0500     1            False   \n",
      "\n",
      "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
      "0                        0                         0   \n",
      "1                        1                         0   \n",
      "2                        0                         0   \n",
      "3                        0                         0   \n",
      "4                        0                         0   \n",
      "\n",
      "   embarked_from_southampton  \n",
      "0                          1  \n",
      "1                          0  \n",
      "2                          1  \n",
      "3                          1  \n",
      "4                          1  \n",
      "(891, 10)\n",
      "(891,)\n",
      "survived                     0\n",
      "pclass                       0\n",
      "age                          0\n",
      "sibsp                        0\n",
      "parch                        0\n",
      "fare                         0\n",
      "male                         0\n",
      "age_was_missing              0\n",
      "embarked_from_cherbourg      0\n",
      "embarked_from_queenstown     0\n",
      "embarked_from_southampton    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data and have a quick look at the results.\n",
    "df = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "X = df.drop(columns=['survived'])\n",
    "y = df['survived']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T02:32:36.531116Z",
     "start_time": "2019-02-02T02:32:30.226821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass   age  sibsp  parch     fare  male  age_was_missing  \\\n",
      "0         0       3  22.0      1      0   7.2500     1            False   \n",
      "1         1       1  38.0      1      0  71.2833     0            False   \n",
      "2         1       3  26.0      0      0   7.9250     0            False   \n",
      "3         1       1  35.0      1      0  53.1000     0            False   \n",
      "4         0       3  35.0      0      0   8.0500     1            False   \n",
      "\n",
      "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
      "0                        0                         0   \n",
      "1                        1                         0   \n",
      "2                        0                         0   \n",
      "3                        0                         0   \n",
      "4                        0                         0   \n",
      "\n",
      "   embarked_from_southampton  \n",
      "0                          1  \n",
      "1                          0  \n",
      "2                          1  \n",
      "3                          1  \n",
      "4                          1  \n",
      "(891, 10)\n",
      "(891,)\n",
      "survived                     0\n",
      "pclass                       0\n",
      "age                          0\n",
      "sibsp                        0\n",
      "parch                        0\n",
      "fare                         0\n",
      "male                         0\n",
      "age_was_missing              0\n",
      "embarked_from_cherbourg      0\n",
      "embarked_from_queenstown     0\n",
      "embarked_from_southampton    0\n",
      "dtype: int64\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Epoch 1/200\n",
      "891/891 [==============================] - 0s 298us/step - loss: 0.8075 - acc: 0.6442\n",
      "Epoch 2/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.6747 - acc: 0.6734\n",
      "Epoch 3/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.5816 - acc: 0.7037\n",
      "Epoch 4/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.5899 - acc: 0.6914\n",
      "Epoch 5/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.5760 - acc: 0.6947\n",
      "Epoch 6/200\n",
      "891/891 [==============================] - 0s 28us/step - loss: 0.5272 - acc: 0.7306\n",
      "Epoch 7/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.5442 - acc: 0.7430\n",
      "Epoch 8/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.5232 - acc: 0.7508\n",
      "Epoch 9/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4916 - acc: 0.7733\n",
      "Epoch 10/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.5065 - acc: 0.7598\n",
      "Epoch 11/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4853 - acc: 0.7823\n",
      "Epoch 12/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4866 - acc: 0.7811\n",
      "Epoch 13/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4669 - acc: 0.8070\n",
      "Epoch 14/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4726 - acc: 0.7811\n",
      "Epoch 15/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4892 - acc: 0.7890\n",
      "Epoch 16/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4824 - acc: 0.7811\n",
      "Epoch 17/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4527 - acc: 0.8070\n",
      "Epoch 18/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4670 - acc: 0.7901\n",
      "Epoch 19/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4476 - acc: 0.8036\n",
      "Epoch 20/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4538 - acc: 0.8092\n",
      "Epoch 21/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4470 - acc: 0.8025\n",
      "Epoch 22/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.5216 - acc: 0.7688\n",
      "Epoch 23/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4665 - acc: 0.7969\n",
      "Epoch 24/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4429 - acc: 0.8081\n",
      "Epoch 25/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4473 - acc: 0.8081\n",
      "Epoch 26/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4418 - acc: 0.8126\n",
      "Epoch 27/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4426 - acc: 0.8148\n",
      "Epoch 28/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4429 - acc: 0.8137\n",
      "Epoch 29/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4434 - acc: 0.8103\n",
      "Epoch 30/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4663 - acc: 0.7912\n",
      "Epoch 31/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4358 - acc: 0.8092\n",
      "Epoch 32/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4606 - acc: 0.7969\n",
      "Epoch 33/200\n",
      "891/891 [==============================] - 0s 34us/step - loss: 0.4485 - acc: 0.7969\n",
      "Epoch 34/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4402 - acc: 0.8092\n",
      "Epoch 35/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4478 - acc: 0.7980\n",
      "Epoch 36/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4463 - acc: 0.8070\n",
      "Epoch 37/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4519 - acc: 0.8013\n",
      "Epoch 38/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4301 - acc: 0.8182\n",
      "Epoch 39/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4374 - acc: 0.8036\n",
      "Epoch 40/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4647 - acc: 0.8013\n",
      "Epoch 41/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4462 - acc: 0.8103\n",
      "Epoch 42/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4243 - acc: 0.8058\n",
      "Epoch 43/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4365 - acc: 0.8148\n",
      "Epoch 44/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4633 - acc: 0.8070\n",
      "Epoch 45/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4429 - acc: 0.8126\n",
      "Epoch 46/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4332 - acc: 0.8070\n",
      "Epoch 47/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4344 - acc: 0.8148\n",
      "Epoch 48/200\n",
      "891/891 [==============================] - 0s 26us/step - loss: 0.4537 - acc: 0.7890\n",
      "Epoch 49/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4513 - acc: 0.8159\n",
      "Epoch 50/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4438 - acc: 0.8002\n",
      "Epoch 51/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4616 - acc: 0.7856\n",
      "Epoch 52/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4337 - acc: 0.8126\n",
      "Epoch 53/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4278 - acc: 0.8058\n",
      "Epoch 54/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4290 - acc: 0.8148\n",
      "Epoch 55/200\n",
      "891/891 [==============================] - 0s 33us/step - loss: 0.4231 - acc: 0.8114\n",
      "Epoch 56/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4144 - acc: 0.8137\n",
      "Epoch 57/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4128 - acc: 0.8238\n",
      "Epoch 58/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4191 - acc: 0.8103\n",
      "Epoch 59/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4224 - acc: 0.8182\n",
      "Epoch 60/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4277 - acc: 0.8137\n",
      "Epoch 61/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4252 - acc: 0.8249\n",
      "Epoch 62/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4432 - acc: 0.8002\n",
      "Epoch 63/200\n",
      "891/891 [==============================] - 0s 33us/step - loss: 0.4274 - acc: 0.8171\n",
      "Epoch 64/200\n",
      "891/891 [==============================] - 0s 36us/step - loss: 0.4442 - acc: 0.8114\n",
      "Epoch 65/200\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.4263 - acc: 0.8204\n",
      "Epoch 66/200\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.4212 - acc: 0.8260\n",
      "Epoch 67/200\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.4332 - acc: 0.8238\n",
      "Epoch 68/200\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.4190 - acc: 0.8171\n",
      "Epoch 69/200\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.4124 - acc: 0.8193\n",
      "Epoch 70/200\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.4175 - acc: 0.8171\n",
      "Epoch 71/200\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.4745 - acc: 0.7935\n",
      "Epoch 72/200\n",
      "891/891 [==============================] - 0s 35us/step - loss: 0.4571 - acc: 0.8103\n",
      "Epoch 73/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4210 - acc: 0.8148\n",
      "Epoch 74/200\n",
      "891/891 [==============================] - 0s 26us/step - loss: 0.4343 - acc: 0.8103\n",
      "Epoch 75/200\n",
      "891/891 [==============================] - 0s 25us/step - loss: 0.4553 - acc: 0.7946\n",
      "Epoch 76/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4228 - acc: 0.8070\n",
      "Epoch 77/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4060 - acc: 0.8204\n",
      "Epoch 78/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4076 - acc: 0.8272\n",
      "Epoch 79/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4060 - acc: 0.8249\n",
      "Epoch 80/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4121 - acc: 0.8182\n",
      "Epoch 81/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4065 - acc: 0.8148\n",
      "Epoch 82/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4609 - acc: 0.8070\n",
      "Epoch 83/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4109 - acc: 0.8260\n",
      "Epoch 84/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4178 - acc: 0.8159\n",
      "Epoch 85/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4088 - acc: 0.8193\n",
      "Epoch 86/200\n",
      "891/891 [==============================] - 0s 33us/step - loss: 0.4257 - acc: 0.8171\n",
      "Epoch 87/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4418 - acc: 0.8114\n",
      "Epoch 88/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4176 - acc: 0.8227\n",
      "Epoch 89/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4421 - acc: 0.8070\n",
      "Epoch 90/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4129 - acc: 0.8126\n",
      "Epoch 91/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4042 - acc: 0.8182\n",
      "Epoch 92/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4493 - acc: 0.8036\n",
      "Epoch 93/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4105 - acc: 0.8159\n",
      "Epoch 94/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4078 - acc: 0.8182\n",
      "Epoch 95/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4182 - acc: 0.8227\n",
      "Epoch 96/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4258 - acc: 0.8148\n",
      "Epoch 97/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4368 - acc: 0.8148\n",
      "Epoch 98/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4459 - acc: 0.8126\n",
      "Epoch 99/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4069 - acc: 0.8137\n",
      "Epoch 100/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4201 - acc: 0.8249\n",
      "Epoch 101/200\n",
      "891/891 [==============================] - 0s 34us/step - loss: 0.4074 - acc: 0.8238\n",
      "Epoch 102/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4126 - acc: 0.8159\n",
      "Epoch 103/200\n",
      "891/891 [==============================] - 0s 27us/step - loss: 0.4054 - acc: 0.8193\n",
      "Epoch 104/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4040 - acc: 0.8249\n",
      "Epoch 105/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4072 - acc: 0.8339\n",
      "Epoch 106/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.3995 - acc: 0.8316\n",
      "Epoch 107/200\n",
      "891/891 [==============================] - 0s 26us/step - loss: 0.4166 - acc: 0.8137\n",
      "Epoch 108/200\n",
      "891/891 [==============================] - 0s 26us/step - loss: 0.4006 - acc: 0.8260\n",
      "Epoch 109/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4138 - acc: 0.8171\n",
      "Epoch 110/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4438 - acc: 0.8182\n",
      "Epoch 111/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4121 - acc: 0.8204\n",
      "Epoch 112/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4112 - acc: 0.8182\n",
      "Epoch 113/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4828 - acc: 0.8002\n",
      "Epoch 114/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4208 - acc: 0.8249\n",
      "Epoch 115/200\n",
      "891/891 [==============================] - 0s 28us/step - loss: 0.3953 - acc: 0.8294\n",
      "Epoch 116/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3999 - acc: 0.8283\n",
      "Epoch 117/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4020 - acc: 0.8283\n",
      "Epoch 118/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4064 - acc: 0.8193\n",
      "Epoch 119/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4160 - acc: 0.8137\n",
      "Epoch 120/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3947 - acc: 0.8272\n",
      "Epoch 121/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4109 - acc: 0.8148\n",
      "Epoch 122/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4131 - acc: 0.8238\n",
      "Epoch 123/200\n",
      "891/891 [==============================] - 0s 28us/step - loss: 0.3912 - acc: 0.8294\n",
      "Epoch 124/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3973 - acc: 0.8204\n",
      "Epoch 125/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3986 - acc: 0.8294\n",
      "Epoch 126/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4007 - acc: 0.8283\n",
      "Epoch 127/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4011 - acc: 0.8249\n",
      "Epoch 128/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4020 - acc: 0.8193\n",
      "Epoch 129/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.3890 - acc: 0.8350\n",
      "Epoch 130/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3914 - acc: 0.8283\n",
      "Epoch 131/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3968 - acc: 0.8215\n",
      "Epoch 132/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4083 - acc: 0.8193\n",
      "Epoch 133/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3903 - acc: 0.8227\n",
      "Epoch 134/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3924 - acc: 0.8350\n",
      "Epoch 135/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4689 - acc: 0.7957\n",
      "Epoch 136/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4506 - acc: 0.8036\n",
      "Epoch 137/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3978 - acc: 0.8249\n",
      "Epoch 138/200\n",
      "891/891 [==============================] - 0s 28us/step - loss: 0.4283 - acc: 0.8103\n",
      "Epoch 139/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4363 - acc: 0.8238\n",
      "Epoch 140/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4084 - acc: 0.8148\n",
      "Epoch 141/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4090 - acc: 0.8294\n",
      "Epoch 142/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3920 - acc: 0.8316\n",
      "Epoch 143/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4105 - acc: 0.8249\n",
      "Epoch 144/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3919 - acc: 0.8283\n",
      "Epoch 145/200\n",
      "891/891 [==============================] - 0s 34us/step - loss: 0.4021 - acc: 0.8171\n",
      "Epoch 146/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3966 - acc: 0.8204\n",
      "Epoch 147/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4054 - acc: 0.8159\n",
      "Epoch 148/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3919 - acc: 0.8294\n",
      "Epoch 149/200\n",
      "891/891 [==============================] - 0s 33us/step - loss: 0.4077 - acc: 0.8148\n",
      "Epoch 150/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3903 - acc: 0.8305\n",
      "Epoch 151/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.3946 - acc: 0.8238\n",
      "Epoch 152/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4066 - acc: 0.8193\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891/891 [==============================] - 0s 32us/step - loss: 0.3997 - acc: 0.8047\n",
      "Epoch 154/200\n",
      "891/891 [==============================] - 0s 33us/step - loss: 0.3941 - acc: 0.8249\n",
      "Epoch 155/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3876 - acc: 0.8272\n",
      "Epoch 156/200\n",
      "891/891 [==============================] - 0s 33us/step - loss: 0.3983 - acc: 0.8294\n",
      "Epoch 157/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4133 - acc: 0.8137\n",
      "Epoch 158/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3923 - acc: 0.8193\n",
      "Epoch 159/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3891 - acc: 0.8193\n",
      "Epoch 160/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3856 - acc: 0.8305\n",
      "Epoch 161/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3964 - acc: 0.8249\n",
      "Epoch 162/200\n",
      "891/891 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8137\n",
      "Epoch 163/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3891 - acc: 0.8294\n",
      "Epoch 164/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.3895 - acc: 0.8361\n",
      "Epoch 165/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3903 - acc: 0.8193\n",
      "Epoch 166/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3929 - acc: 0.8294\n",
      "Epoch 167/200\n",
      "891/891 [==============================] - 0s 28us/step - loss: 0.3902 - acc: 0.8272\n",
      "Epoch 168/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4008 - acc: 0.8137\n",
      "Epoch 169/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.3913 - acc: 0.8272\n",
      "Epoch 170/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3824 - acc: 0.8316\n",
      "Epoch 171/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3971 - acc: 0.8260\n",
      "Epoch 172/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3846 - acc: 0.8260\n",
      "Epoch 173/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3813 - acc: 0.8294\n",
      "Epoch 174/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3844 - acc: 0.8316\n",
      "Epoch 175/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4094 - acc: 0.8204\n",
      "Epoch 176/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.4051 - acc: 0.8193\n",
      "Epoch 177/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4053 - acc: 0.8215\n",
      "Epoch 178/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.3878 - acc: 0.8328\n",
      "Epoch 179/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3991 - acc: 0.8148\n",
      "Epoch 180/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4152 - acc: 0.8036\n",
      "Epoch 181/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3894 - acc: 0.8294\n",
      "Epoch 182/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.4078 - acc: 0.8171\n",
      "Epoch 183/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.3833 - acc: 0.8260\n",
      "Epoch 184/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.3857 - acc: 0.8272\n",
      "Epoch 185/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.3819 - acc: 0.8350\n",
      "Epoch 186/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3960 - acc: 0.8238\n",
      "Epoch 187/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.3890 - acc: 0.8171\n",
      "Epoch 188/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.3887 - acc: 0.8328\n",
      "Epoch 189/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3971 - acc: 0.8238\n",
      "Epoch 190/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.3883 - acc: 0.8272\n",
      "Epoch 191/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3856 - acc: 0.8283\n",
      "Epoch 192/200\n",
      "891/891 [==============================] - 0s 32us/step - loss: 0.3902 - acc: 0.8193\n",
      "Epoch 193/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4087 - acc: 0.8350\n",
      "Epoch 194/200\n",
      "891/891 [==============================] - 0s 28us/step - loss: 0.3961 - acc: 0.8126\n",
      "Epoch 195/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3739 - acc: 0.8384\n",
      "Epoch 196/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3822 - acc: 0.8339\n",
      "Epoch 197/200\n",
      "891/891 [==============================] - 0s 31us/step - loss: 0.3904 - acc: 0.8227\n",
      "Epoch 198/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.3900 - acc: 0.8339\n",
      "Epoch 199/200\n",
      "891/891 [==============================] - 0s 29us/step - loss: 0.4321 - acc: 0.8126\n",
      "Epoch 200/200\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.4129 - acc: 0.8193\n",
      "0.11 - No\n",
      "0.74 - Yes\n",
      "0.77 - Yes\n",
      "1.00 - Yes\n",
      "0.11 - No\n",
      "0.09 - No\n",
      "0.05 - No\n",
      "0.48 - No\n",
      "0.12 - No\n",
      "0.96 - Yes\n",
      "0.10 - No\n",
      "0.04 - No\n",
      "0.12 - No\n",
      "0.13 - No\n",
      "0.09 - No\n",
      "0.04 - No\n",
      "0.53 - Yes\n",
      "0.10 - No\n",
      "0.04 - No\n",
      "0.03 - No\n",
      "0.93 - Yes\n",
      "0.09 - No\n",
      "0.05 - No\n",
      "0.50 - No\n",
      "0.10 - No\n",
      "0.04 - No\n",
      "0.20 - No\n",
      "0.97 - Yes\n",
      "0.06 - No\n",
      "0.96 - Yes\n",
      "0.67 - Yes\n",
      "0.95 - Yes\n",
      "0.08 - No\n",
      "0.10 - No\n",
      "0.10 - No\n",
      "0.94 - Yes\n",
      "0.08 - No\n",
      "0.06 - No\n",
      "0.23 - No\n",
      "0.37 - No\n",
      "0.10 - No\n",
      "0.14 - No\n",
      "0.96 - Yes\n",
      "0.07 - No\n",
      "0.09 - No\n",
      "0.05 - No\n",
      "0.00 - No\n",
      "0.08 - No\n",
      "0.14 - No\n",
      "0.97 - Yes\n",
      "0.12 - No\n",
      "0.00 - No\n",
      "0.75 - Yes\n",
      "0.94 - Yes\n",
      "0.78 - Yes\n",
      "0.57 - Yes\n",
      "0.91 - Yes\n",
      "0.25 - No\n",
      "0.73 - Yes\n",
      "0.08 - No\n",
      "0.02 - No\n",
      "0.10 - No\n",
      "0.94 - Yes\n",
      "0.00 - No\n",
      "0.08 - No\n",
      "0.85 - Yes\n",
      "0.78 - Yes\n",
      "0.35 - No\n",
      "0.06 - No\n",
      "0.93 - Yes\n",
      "0.10 - No\n",
      "0.91 - Yes\n",
      "0.14 - No\n",
      "0.04 - No\n",
      "0.84 - Yes\n",
      "0.82 - Yes\n",
      "0.08 - No\n",
      "0.10 - No\n",
      "0.06 - No\n",
      "0.98 - Yes\n",
      "0.79 - Yes\n",
      "0.09 - No\n",
      "0.52 - Yes\n",
      "0.12 - No\n",
      "0.11 - No\n",
      "0.11 - No\n",
      "0.08 - No\n",
      "0.97 - Yes\n",
      "0.38 - No\n",
      "0.41 - No\n",
      "0.07 - No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "KERAS_TITANIC_MODEL_H_ = 'keras_titanic_model.h5'\n",
    "\n",
    "df = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "X = df.drop(columns=['survived'])\n",
    "y = df['survived']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "n_input_features = X.shape[1]\n",
    "\n",
    "# create a one-hot encoded version of the target output\n",
    "y_categorical = to_categorical(y)\n",
    "print(y_categorical)\n",
    "\n",
    "if os.path.exists(KERAS_TITANIC_MODEL_H_):\n",
    "    model = load_model(KERAS_TITANIC_MODEL_H_)\n",
    "else:\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer 0 for Inputs, and 100 node hidden layer\n",
    "    model.add(Dense(100, activation='relu', input_shape=(n_input_features,)))\n",
    "\n",
    "    # Add the output layer.  2 because 2 possible outcomes\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X, y_categorical, epochs=200)\n",
    "\n",
    "    # Save model for later\n",
    "    model.save(KERAS_TITANIC_MODEL_H_)\n",
    "\n",
    "\"pclass,age,sibsp,parch,fare,male,age_was_missing,embarked_from_cherbourg,embarked_from_queenstown,embarked_from_southampton\"\n",
    "pred_data = np.array([[2, 34.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "                      [2, 31.0, 1, 1, 26.25, 0, False, 0, 0, 1],\n",
    "                      [1, 11.0, 1, 2, 120.0, 1, False, 0, 0, 1],\n",
    "                      [3, 0.42, 0, 1, 8.5167, 1, False, 1, 0, 0],\n",
    "                      [3, 27.0, 0, 0, 6.975, 1, False, 0, 0, 1],\n",
    "                      [3, 31.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "                      [1, 39.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "                      [3, 18.0, 0, 0, 7.775, 0, False, 0, 0, 1],\n",
    "                      [2, 39.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "                      [1, 33.0, 1, 0, 53.1, 0, False, 0, 0, 1],\n",
    "                      [3, 26.0, 0, 0, 7.8875, 1, False, 0, 0, 1],\n",
    "                      [3, 39.0, 0, 0, 24.15, 1, False, 0, 0, 1],\n",
    "                      [2, 35.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "                      [3, 6.0, 4, 2, 31.275, 0, False, 0, 0, 1],\n",
    "                      [3, 30.5, 0, 0, 8.05, 1, False, 0, 0, 1],\n",
    "                      [1, 29.69911764705882, 0, 0, 0.0, 1, True, 0, 0, 1],\n",
    "                      [3, 23.0, 0, 0, 7.925, 0, False, 0, 0, 1],\n",
    "                      [2, 31.0, 1, 1, 37.0042, 1, False, 1, 0, 0],\n",
    "                      [3, 43.0, 0, 0, 6.45, 1, False, 0, 0, 1],\n",
    "                      [3, 10.0, 3, 2, 27.9, 1, False, 0, 0, 1],\n",
    "                      [1, 52.0, 1, 1, 93.5, 0, False, 0, 0, 1],\n",
    "                      [3, 27.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "                      [1, 38.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "                      [3, 27.0, 0, 1, 12.475, 0, False, 0, 0, 1],\n",
    "                      [3, 2.0, 4, 1, 39.6875, 1, False, 0, 0, 1],\n",
    "                      [3, 29.69911764705882, 0, 0, 6.95, 1, True, 0, 1, 0],\n",
    "                      [3, 29.69911764705882, 0, 0, 56.4958, 1, True, 0, 0, 1],\n",
    "                      [2, 1.0, 0, 2, 37.0042, 1, False, 1, 0, 0],\n",
    "                      [3, 29.69911764705882, 0, 0, 7.75, 1, True, 0, 1, 0],\n",
    "                      [1, 62.0, 0, 0, 80.0, 0, False, 0, 0, 0],\n",
    "                      [3, 15.0, 1, 0, 14.4542, 0, False, 1, 0, 0],\n",
    "                      [2, 0.83, 1, 1, 18.75, 1, False, 0, 0, 1],\n",
    "                      [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "                      [3, 23.0, 0, 0, 7.8542, 1, False, 0, 0, 1],\n",
    "                      [3, 18.0, 0, 0, 8.3, 1, False, 0, 0, 1],\n",
    "                      [1, 39.0, 1, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "                      [3, 21.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "                      [3, 29.69911764705882, 0, 0, 8.05, 1, True, 0, 0, 1],\n",
    "                      [3, 32.0, 0, 0, 56.4958, 1, False, 0, 0, 1],\n",
    "                      [1, 29.69911764705882, 0, 0, 29.7, 1, True, 1, 0, 0],\n",
    "                      [3, 20.0, 0, 0, 7.925, 1, False, 0, 0, 1],\n",
    "                      [2, 16.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "                      [1, 30.0, 0, 0, 31.0, 0, False, 1, 0, 0],\n",
    "                      [3, 34.5, 0, 0, 6.4375, 1, False, 1, 0, 0],\n",
    "                      [3, 17.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "                      [3, 42.0, 0, 0, 7.55, 1, False, 0, 0, 1],\n",
    "                      [3, 29.69911764705882, 8, 2, 69.55, 1, True, 0, 0, 1],\n",
    "                      [3, 35.0, 0, 0, 7.8958, 1, False, 1, 0, 0],\n",
    "                      [2, 28.0, 0, 1, 33.0, 1, False, 0, 0, 1],\n",
    "                      [1, 29.69911764705882, 1, 0, 89.1042, 0, True, 1, 0, 0],\n",
    "                      [3, 4.0, 4, 2, 31.275, 1, False, 0, 0, 1],\n",
    "                      [3, 74.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "                      [3, 9.0, 1, 1, 15.2458, 0, False, 1, 0, 0],\n",
    "                      [1, 16.0, 0, 1, 39.4, 0, False, 0, 0, 1],\n",
    "                      [2, 44.0, 1, 0, 26.0, 0, False, 0, 0, 1],\n",
    "                      [3, 18.0, 0, 1, 9.35, 0, False, 0, 0, 1],\n",
    "                      [1, 45.0, 1, 1, 164.8667, 0, False, 0, 0, 1],\n",
    "                      [1, 51.0, 0, 0, 26.55, 1, False, 0, 0, 1],\n",
    "                      [3, 24.0, 0, 3, 19.2583, 0, False, 1, 0, 0],\n",
    "                      [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "                      [3, 41.0, 2, 0, 14.1083, 1, False, 0, 0, 1],\n",
    "                      [2, 21.0, 1, 0, 11.5, 1, False, 0, 0, 1],\n",
    "                      [1, 48.0, 0, 0, 25.9292, 0, False, 0, 0, 1],\n",
    "                      [3, 29.69911764705882, 8, 2, 69.55, 0, True, 0, 0, 1],\n",
    "                      [2, 24.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "                      [2, 42.0, 0, 0, 13.0, 0, False, 0, 0, 1],\n",
    "                      [2, 27.0, 1, 0, 13.8583, 0, False, 1, 0, 0],\n",
    "                      [1, 31.0, 0, 0, 50.4958, 1, False, 0, 0, 1],\n",
    "                      [3, 29.69911764705882, 0, 0, 9.5, 1, True, 0, 0, 1],\n",
    "                      [3, 4.0, 1, 1, 11.1333, 1, False, 0, 0, 1],\n",
    "                      [3, 26.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "                      [1, 47.0, 1, 1, 52.5542, 0, False, 0, 0, 1],\n",
    "                      [1, 33.0, 0, 0, 5.0, 1, False, 0, 0, 1],\n",
    "                      [3, 47.0, 0, 0, 9.0, 1, False, 0, 0, 1],\n",
    "                      [2, 28.0, 1, 0, 24.0, 0, False, 1, 0, 0],\n",
    "                      [3, 15.0, 0, 0, 7.225, 0, False, 1, 0, 0],\n",
    "                      [3, 20.0, 0, 0, 9.8458, 1, False, 0, 0, 1],\n",
    "                      [3, 19.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "                      [3, 29.69911764705882, 0, 0, 7.8958, 1, True, 0, 0, 1],\n",
    "                      [1, 56.0, 0, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "                      [2, 25.0, 0, 1, 26.0, 0, False, 0, 0, 1],\n",
    "                      [3, 33.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "                      [3, 22.0, 0, 0, 10.5167, 0, False, 0, 0, 1],\n",
    "                      [2, 28.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "                      [3, 25.0, 0, 0, 7.05, 1, False, 0, 0, 1],\n",
    "                      [3, 39.0, 0, 5, 29.125, 0, False, 0, 1, 0],\n",
    "                      [2, 27.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "                      [1, 19.0, 0, 0, 30.0, 0, False, 0, 0, 1],\n",
    "                      [3, 29.69911764705882, 1, 2, 23.45, 0, True, 0, 0, 1],\n",
    "                      [1, 26.0, 0, 0, 30.0, 1, False, 1, 0, 0],\n",
    "                      [3, 32.0, 0, 0, 7.75, 1, False, 0, 1, 0]])\n",
    "\n",
    "# your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.\n",
    "y_pred = model.predict(pred_data)\n",
    "probabilty_true = y_pred[:, 1]\n",
    "for p in probabilty_true:\n",
    "    survived = 'No'\n",
    "    if p > 0.5:\n",
    "        survived = 'Yes'\n",
    "    print(f'{p:.2f} - {survived}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "Model optimization can be difficult because the model is simultaneously optimize 1000s of weights and parameters in a complex network of relationships.\n",
    "\n",
    "It is possible, even very likely, that some updates will not measurably improve the model and therefore new weights and parameters will have to be tried.\n",
    "\n",
    "These updates might take too long, meaning the learning rate is very small, or will never converge on an optimal solution meaning the learning rate is too high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Learning Rates and Stochastic Gradient Descent\n",
    "\n",
    "You can specify the learning rate when you specify a SGD optimizer.\n",
    "\n",
    "```python\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model.compile(optimizer=SGD(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "By specifying the optimizer this way, we can adjust the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dying Neuron Problem\n",
    "\n",
    "This is when a node starts to get negative inputs, it may continue to get negative inputs and will always be zero.\n",
    "\n",
    "There are different activations functions, but for now just be aware this can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Validation\n",
    "\n",
    "It is common to create training testing datasets.  In many cases we use crossvalidation to go through a dataset with a new hold out each time to test how the model setup is generalizing to unseen data.\n",
    "\n",
    "However, in a deep learning is typically run on large datasets and so to use crossvalidation would just not be feasible because the computational expense is too large.\n",
    "\n",
    "Keras exposes a *validation_split* parameter to handle this scenario without the overhead.\n",
    "\n",
    "The parameter is part of the *fit* method like:\n",
    "```python\n",
    "model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4)\n",
    "```\n",
    "\n",
    "When you add *validation_split* will see in the output:\n",
    "\n",
    "```python\n",
    "0s 41us/step - loss: 0.4321 - acc: 0.8122 - val_loss: 0.4278 - val_acc: 0.8060\n",
    "Epoch 56/2000```\n",
    "\n",
    "\n",
    "**acc** is the accuracy of a batch of training data and **val_acc** is the accuracy of a batch of testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "When the model validation does not show the model is improving then stop the training.  This allows us to specify a complex model architecture knowing that once the model stops improving the training will stop.\n",
    "\n",
    "```python\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "model.fit(X, y_categorical, validation_split=0.3, callbacks=[early_stopping_monitor], epochs=2000)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The main take aways are:\n",
    "\n",
    "- Why does deep learning work?\n",
    "\n",
    "Captures the non-linear interactions between features from the data, minimizing the need for exhaustive feature engineering.\n",
    "\n",
    "- Understanding forward propagation.\n",
    "\n",
    "The application of an activation function to the dot product of the input values and weights into a node.\n",
    "\n",
    "- Understanding back propagation\n",
    "\n",
    "How the system updates the weights of the network by pushing the error back through the network.\n",
    "\n",
    "\n",
    "- Understanding how to use Keras with a tensorflow backend\n",
    "\n",
    "How to setup a network, save and load the model, compile the model and fit the network.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
